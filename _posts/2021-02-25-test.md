---
title:  테스트용으로 글과 이미지, 수식, 코드를 모두 넣어봤습니다.
tags:
  - Test
images:
---

## **Lecture 2.1 - Linear Regression With One Variable**

&nbsp;&nbsp;&nbsp;&nbsp;세상엔 다양한 집들이 있을 것이다. 이 집들을 x축은 집의 면적, y축은 집의 가격에 해당하는 직교 좌표 위에 데이터로 나타난다고 가정해보자. 대략적으로 양의 상관관계가 있는 그래프가 나올 것이다. 여기서 집의 면적과 가격 두 값에 대한 학습을 한다고 가정하자.<br>
&nbsp;&nbsp;&nbsp;&nbsp;사전에 수집한 집값 정보라는 정답이 존재하므로 supervised learning에 해당하며, 추정하고자 하는 값이 실수값이므로 regression problem이다. 만약 일차함수에 비슷한 직선 그래프가 나온다면 linear regression problem이 된다.

<center>m: # of training examples</center>
<center>x’s = “input” variable / features</center>
<center>y’s = “output” variable / “target” variable</center>
로 정의되었을 때,

<center>(x, y) - one training example<br></center>
<center>(x<sup>(i)</sup>, y<sup>(i)</sup>) - i<sub>th</sub> training example</center>
와 같은 방식으로 접근할 수 있다.

&nbsp;&nbsp;&nbsp;&nbsp;일단 기본 구조는 training set에 대해, 알고리즘을 도출하여 알맞는 가설(hypothesis)을 만들고 검증하는 것이다. 여기서 x값은 집의 면적, y값은 집의 가격이 될 것이다. 가설은 x값으로부터 y값을 도출하는 특정 식이 될 것이다.

&nbsp;&nbsp;&nbsp;&nbsp;그렇다면 우리는 가설을 어떻게 세우는가? 가설은 어떤 형태의 함수로도 가능하지만, 일단 선형함수가 가장 많이 쓰인다.<br>
<center>$h_{θ}(x) = θ_{0}+θ_{1}x$ 와 같이 말이다.</center><br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.2 - Cost Function**</span>

&nbsp;&nbsp;&nbsp;&nbsp;가설을 $$h_{θ}(x) = θ_{0}+θ_{1}x$$와 같은 선형함수로 세웠다고 치자. 그러면 우리는 이 가설이 잘 맞아떨어지는 지 어떻게 확인할 수 있을까?
일단 여기서 $$θ_{1}$$, $$θ_{2}$$이 두 변수(parameter)로써 존재하므로, 이들에 대한 타당성이 증명되어야 할 것이다. 그리고 이를 평가할 수 있는 수치가 바로 cost function이다.



&nbsp;&nbsp;&nbsp;&nbsp;아래 식들은 cost function을 구하는 과정이다. 핵심 아이디어는 $$θ_{i}$$를 잘 골라서 $$h_{θ}(x)$$이 $$y$$에 가깝게 하는 것이다. $(1)$ 한 sample에서 sample값($y$)과 가설값($x$)의 차이를 수치화하고
 $(2)$ 이를 sample 수 만큼 더한 뒤
 $(3)$ sample 수 만큼 나누어서 평균을 구한다. 2를 더 나눈 것은 수학적 편의를 위한 것이라고 한다.


$$(h_{θ}(x)-y)^2\label{basic01}\tag{1}$$
$$\sum\limits_{i=1}^{m}{(h_{θ}(x^{(i)})-y^{(i)})^2}\tag{2}$$ 
$$cost function = \left(\cfrac{1}{2m}\right)\sum\limits_{i=1}^{m}{(h_{θ}(x^{(i)})-y^{(i)})^2}\tag{3}$$ <br>




## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.3 - Cost Function Intuition**</span>


&nbsp;&nbsp;&nbsp;&nbsp;직관적인 접근을 위해 $θ_{0}$를 $0$으로 두면, $h_{θ}(x) = θ_{1}x$ 이다. 목표도 $J(θ_{1})$를 최소화하는 것이 된다. 가로축을 $θ_{1}$, 세로축을 $J(θ_{1})$으로 한 그래프를 그린다면, 아래로 볼록한 그래프가 그려지게 되는데,
여기서 $J(θ_{1})$값이 최소가 되는 지점이 바로 Cost function이 가장 최소인, 즉 가장 잘 들어맞는 가설 지점이 되는 것이다.<br>

&nbsp;&nbsp;&nbsp;&nbsp;아래는 내가 직접 그린 그림으로, {(1,1),(2,2),(3,3)}의 training set가 있을 때의 조건이다.<br>
![tansex1](/images/posts/2020-02-27-tensorflow1/tensex1.png){:width="60%"}{: .center}

&nbsp;&nbsp;&nbsp;&nbsp;<span style="line-height:160%; font-size: 19px; color:#519d9e;">**Lecture 2.3 관련 실습**</span>

&nbsp;&nbsp;&nbsp;&nbsp;위 그림을 만들 수 있는 코딩을 아래 포스트에 정리해두었다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;[(바로가기)Tensorflow 실습 1](https://now-man.github.io/posts/tensorflow1/)<br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.4 - Cost Function Intuition #2**</span>

&nbsp;&nbsp;&nbsp;&nbsp;위에서는 parameter를 하나로 고정했지만, 일반적인 선형함수($h_{θ}(x) = θ_{0}+θ_{1}x$)에서는 parameter가 두 개다. 이 두 parameter에 대해 $J(θ_{0}+θ_{1})$ 그래프를 그리면 다음과 같이 3차원의 그래프가 나온다. 이번에도 $J(θ_{0}+θ_{1})$ 값이 가장 낮은 지점이 가장 일치하는 가설임은 동일하다.<br>
![tumor2](/images/posts/2020-02-27-ai2/cost3d.png){:width="60%"}{: .center}

&nbsp;&nbsp;&nbsp;&nbsp;$θ_{0}$,$θ_{1}$을 축으로 삼고,$J(θ_{0}+θ_{1})$값이 같은 선을 이어주어 2차원 그래프를 그리면 등고선 모양이 된다. 여기서도 등고선의 가장 중앙이 가장 일치하는 가설이다.<br>
![tumor2](/images/posts/2020-02-27-ai2/cost2d.png){:width="60%"}{: .center}


## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.5 - Gradient descent**</span>

&nbsp;&nbsp;&nbsp;&nbsp;이제 cost function에 대해 대충 알았으니, 이를 자동으로 구해주는 알고리즘이 필요하다.
핵심 아이디어는 특정 $θ_{0}$, $θ_{1}$을 찝어서, 이를 조금조금씩 변화시키며 $J(θ_{0}+θ_{1})$의 최소값을 찾아내는 것이다.
$θ_{i}$는 뭘로 시작해도 괜찮지만 $0$으로 시작하는 게 일반적이라고 한다.<br>
&nbsp;&nbsp;&nbsp;&nbsp; 아래의 식이 바로 gradient descent algorithm이다.
여기서 $:=$는 assignment라고 한다. $a:=b$라고 함은 $b$의 값을 $a$에 적용하겠다는 것인데, 예를 들면 $a:=a+1$일 때는 a의 값이
1씩 늘어난다고 보면 된다. $\alpha$는 learning rate인데, $\alpha$가 작을 수록 더 세밀하게 검사한다고 보면 된다.
$\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$은 cost function을 $\theta_j$에 대해 편미분한 값으로써 기울기라 보면 된다.

<center>$\theta_{j}:=\theta_{j}-\alpha\cfrac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad(for\;j = 0\;and\; j = 1)$</center><br>

&nbsp;&nbsp;&nbsp;&nbsp; 위의 식에서 $\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$에 해당하는 값을 더 풀어보면 아래와 같다.
여기서 $\theta_0$과 $\theta_1$은 동시에 업데이트되어야 한다.

\begin{align} \cfrac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) & = \cfrac{\partial}{\partial\theta_{j}}\Biggl[\cfrac{1}{2m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)^2\Biggr]
\\\\ & = \cfrac{\partial}{\partial\theta_{j}}\left[\cfrac{1}{2m}\sum\limits_{i=1}^{m}\left(  (\theta_0 + \theta_{1}x^{(i)})-y^{(i)}  \right)^2\\right]
\end{align}
<br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.6 - Gradient descent Intuition**</span>

&nbsp;&nbsp;&nbsp;&nbsp;$x$축이 $\theta_1$, $y$축이 $J(\theta_1)$로 구성된 그래프가 있다고 하자.
임의의 $\theta_1$ 지점에 해당하는 $J(\theta_1)$ 값을 구하고, 여기에 점을 찍어 이를 지나는 접선을 구한다면,
이 접선의 기울기는 $\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$가 된다.
이 기울기가 양수라면 $\theta_1$ $=$ $\theta_1$ $-$ $\alpha$$*(positive\;number)$가 되니
$\theta_1$ 값이 작아질 수록 최소값에 가까워진다고 볼 수 있다.
기울기가 음수일 때는 $\theta_1$ 값이 커질 수록 최소값에 가까워진다.
여기서 $\alpha$값이 너무 작으면 최소값 도달까지 너무 오래 걸릴 수 있고,
$\alpha$값이 너무 크면 최소값에 이르지 못할 수 있다. 결국 적절한 $\alpha$ 값을 넣어야 한다.

<center><i class="fas fa-quote-left" style="color:#A593E0;"></i>
<span style="color:gray">&nbsp;&nbsp;만약 $J(\theta_1)$가 이미 극솟값에 도달한 상태에서, $\alpha$가 더 업데이트되면 어떻게 될까?&nbsp;&nbsp;</span>
<i class="fas fa-quote-right" style="color:#A593E0;"></i></center>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;$J(\theta_1)$가 극솟값에 도달했다면 $\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$은 $0$이다.
이 말은 이미 극솟값에 도달했다면 $\theta_1$이 바뀌지 않으니 더이상 업데이트되지 않는다는 말이다.
$\alpha$가 바뀌지 않더라도 대부분의 경우 업데이트 될 때마다 기울기가 자동으로 $0$에 가까워지며 극솟값에 가까워진다.

![tumor2](/images/posts/2020-02-27-ai2/localoptima.png){:width="60%"}{: .center}
<br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.7- Gradient descent for Linear Regression**</span>

&nbsp;&nbsp;&nbsp;&nbsp;위에서 구한 식들을 토대로 $\theta_j$에 대한 알고리즘을 구해보면 아래와 같다.

\begin{align} \cfrac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) & = \cfrac{\partial}{\partial\theta_{j}}\Biggl[\cfrac{1}{2m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)^2\Biggr]
\\\\ & = \cfrac{\partial}{\partial\theta_{j}}\left[\cfrac{1}{2m}\sum\limits_{i=1}^{m}\left(  \theta_0 + \theta_{1}x^{(i)}-y^{(i)}  \right)^2\\right]
\end{align}
<br>

\begin{align} j=0:\cfrac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) &= \cfrac{1}{m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)
\\\\ j=1:\cfrac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) & = \cfrac{1}{m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)x^{(i)}
\end{align}
<br>

\begin{align}
\theta_0:&= \theta_0 - \alpha\cfrac{1}{m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)
\\\\\theta_1:&= \theta_1 - \alpha\cfrac{1}{m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)x^{(i)}
\end{align}
<br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.8- What's Next**</span>

&nbsp;&nbsp;&nbsp;&nbsp;지금까지 배운 것에 더해 추가할 수 있는 요소는 무엇이 있을까. 1) $min\;J(\theta_0, \theta_1)$을 gradient descent 없이 구하기
2) 두 개 이상의 변수에 대한 $min\;J(\theta_0, ... , \theta_n)$ 값 구하기 가 있을 것이다. 이제 machine learning을 위해서 linear algebra를 잘 응용해야 할
때가 왔다. 그래도 기본적인 것만 알면 machine learning에 적용할 수 있다고 한다.


&nbsp;&nbsp;&nbsp;&nbsp;*- matrix와 vector가 뭔지*<br>
&nbsp;&nbsp;&nbsp;&nbsp;*- matrix와 vector의 덧셈, 뺄셈, 곱셈<br>
&nbsp;&nbsp;&nbsp;&nbsp;*- matrix의 inverse, transpose<br>
&nbsp;&nbsp;&nbsp;&nbsp;말이다.
<br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· 참고**</span>
유튜브: <https://www.youtube.com/channel/UC5zx8Owijmv-bbhAK6Z9apg><br>
위키독스: <https://wikidocs.net/4213>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script>
<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

{% endhighlight %}